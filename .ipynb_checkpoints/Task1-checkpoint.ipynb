{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from statistics import median, mean\n",
    "from collections import Counter\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartPoleEnv - Version 0.2.0, Noise case: 1\n"
     ]
    }
   ],
   "source": [
    "LR = 1e-3\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "goal_steps = 500\n",
    "EPISODES = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot score over episodes\n",
    "def plot_res(values, title=''):   \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Define the figure\n",
    "    f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n",
    "    f.suptitle(title)\n",
    "    ax[0].plot(values, label='score per run')\n",
    "    ax[0].axhline(100, c='red',ls='--', label='goal')\n",
    "    ax[0].set_xlabel('Episodes')\n",
    "    ax[0].set_ylabel('Reward')\n",
    "    x = range(len(values))\n",
    "    ax[0].legend()\n",
    "    # Calculate the trend\n",
    "    try:\n",
    "        z = np.polyfit(x, values, 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax[0].plot(x,p(x),\"--\", label='trend')\n",
    "    except:\n",
    "        print('')\n",
    "    \n",
    "    # Plot the histogram of results\n",
    "    ax[1].hist(values[-50:])\n",
    "    ax[1].axvline(500, c='red', label='goal')\n",
    "    ax[1].set_xlabel('Scores per Last 50 Episodes')\n",
    "    ax[1].set_ylabel('Frequency')\n",
    "    ax[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(input_size, output_size):\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(24, input_dim=input_size,activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    #model.add(Dropout(0.3))\n",
    "    #model.add(Dense(256,kernel_initializer='normal', activation='relu'))\n",
    "    #model.add(Dropout(0.3))\n",
    "    #model.add(Dense(512,kernel_initializer='normal', activation='relu'))\n",
    "    #model.add(Dropout(0.3))\n",
    "    #model.add(Dense(256,kernel_initializer='normal', activation='relu'))\n",
    "    #model.add(Dropout(0.3))\n",
    "    #model.add(Dense(128,kernel_initializer='normal', activation='relu'))\n",
    "    #model.add(Dropout(0.3))\n",
    "    model.add(Dense(output_size,activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "\n",
    "print(input_size)\n",
    "print(output_size)\n",
    "\n",
    "model = neural_network_model(input_size, output_size)\n",
    "target_model = neural_network_model(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0 #Used to define the exploration range\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "def next_action(state, output_size):\n",
    "    if(np.random.rand() <= epsilon): #Explore: Random action\n",
    "        return random.randrange(output_size)\n",
    "    return np.argmax(model.predict(state.reshape(-1, len(state)))) #Exploitation: Action with best possible reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "\n",
    "\n",
    "def replay(model, training_data, train_size):\n",
    "    global epsilon, epsilon_min, epsilon_decay\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    batch_data = random.sample(training_data, train_size)\n",
    "    for t in batch_data:\n",
    "        state, reward, done, new_state, action = t\n",
    "        \n",
    "        yi = model.predict(state)\n",
    "        \n",
    "        #X.append(state)\n",
    "        #y.append(yi[0])\n",
    "        if(done):\n",
    "            yi[0][action] = reward\n",
    "        else:\n",
    "            a = model.predict(new_state)[0]\n",
    "            t = target_model.predict(new_state)[0]\n",
    "            yi[0][action] = reward + gamma * t[np.argmax(a)]\n",
    "        #state=state.tolist()[0]\n",
    "        #yi=yi.tolist()[0]\n",
    "        model.fit(state, yi, epochs=1, verbose=0)\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    #X = np.array([x[0] for x in training_data])\n",
    "    #y = [int(x[4]) for x in training_data]\n",
    "    #X = np.array(X)\n",
    "    #y = np.array(y)\n",
    "    \n",
    "    #model.fit(X, y, epochs=1, verbose = 0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 0 score : 14.0\n",
      "Game 100 score : 26.0\n",
      "Game 200 score : 22.0\n",
      "Game 300 score : 77.0\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "game_memory = deque(maxlen=100000)\n",
    "\n",
    "train_size = 32\n",
    "\n",
    "for game in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    #env.render()\n",
    "    score = 0\n",
    "    for _ in range(goal_steps):\n",
    "        action = next_action(state, output_size)\n",
    "        \n",
    "        state = state.reshape(-1, len(state))\n",
    "                \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        score += reward\n",
    "        if done:\n",
    "            reward = -reward\n",
    "        else:\n",
    "            reward = reward\n",
    "        \n",
    "        game_memory.append((state, reward, done, new_state.reshape(-1, len(new_state)), action))\n",
    "        \n",
    "        state = new_state\n",
    "        if done: \n",
    "            target_model.set_weights(model.get_weights())\n",
    "            #print(\"episode: {}/{}, score: {}, e: {:.2}\".format(game, EPISODES, _, epsilon))\n",
    "            break\n",
    "            \n",
    "    if(len(game_memory) > train_size):\n",
    "        replay(model, game_memory, train_size)\n",
    "    \n",
    "    if(game % 100 == 0):\n",
    "        print('Game {} score : {}'.format(game, score))\n",
    "        \n",
    "    scores.append(score)\n",
    "    #plot_res(scores)\n",
    "\n",
    "print('Average Score:',sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python mlproject",
   "language": "python",
   "name": "mlproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
