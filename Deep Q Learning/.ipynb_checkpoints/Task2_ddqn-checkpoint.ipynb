{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from statistics import median, mean\n",
    "from collections import Counter\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartPoleEnv - Version 0.2.0, Noise case: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohanish3/anaconda3/envs/mlproject/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/mohanish3/anaconda3/envs/mlproject/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'gym.envs.classic_control.cartpole.CartPoleEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "LR = 1e-3\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot score over episodes\n",
    "def plot_res(values, title=''):   \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Define the figure\n",
    "    f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n",
    "    f.suptitle(title)\n",
    "    ax[0].plot(values, label='score per run')\n",
    "    ax[0].axhline(100, c='red',ls='--', label='goal')\n",
    "    ax[0].set_xlabel('Episodes')\n",
    "    ax[0].set_ylabel('Reward')\n",
    "    x = range(len(values))\n",
    "    ax[0].legend()\n",
    "    # Calculate the trend\n",
    "    try:\n",
    "        z = np.polyfit(x, values, 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax[0].plot(x,p(x),\"--\", label='trend')\n",
    "    except:\n",
    "        print('')\n",
    "    \n",
    "    # Plot the histogram of results\n",
    "    ax[1].hist(values[-50:])\n",
    "    ax[1].axvline(500, c='red', label='goal')\n",
    "    ax[1].set_xlabel('Scores per Last 50 Episodes')\n",
    "    ax[1].set_ylabel('Frequency')\n",
    "    ax[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(input_size, output_size):\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(24, input_dim=input_size,activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(output_size,activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "\n",
    "print(input_size)\n",
    "print(output_size)\n",
    "\n",
    "model = neural_network_model(input_size, output_size)\n",
    "target_model = neural_network_model(input_size, output_size)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(model, training_data, train_size):\n",
    "    global epsilon, epsilon_min, epsilon_decay\n",
    "    \n",
    "    batch_data = random.sample(training_data, train_size)\n",
    "    for t in batch_data:\n",
    "        state, reward, done, new_state, action = t\n",
    "        \n",
    "        yi = model.predict(state)\n",
    "        \n",
    "        if(done):\n",
    "            yi[0][action] = reward\n",
    "        else:\n",
    "            a = model.predict(new_state)[0]\n",
    "            t = target_model.predict(new_state)[0]\n",
    "            yi[0][action] = reward + gamma * t[np.argmax(a)]\n",
    "        model.fit(state, yi, epochs=1, verbose=0)\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_action(state, output_size):\n",
    "    if(np.random.rand() <= epsilon): #Explore: Random action\n",
    "        return random.randrange(output_size)\n",
    "    return np.argmax(model.predict(state)) #Exploitation: Action with best possible reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 0 score : 16.0\n",
      "Game 100 score : 14.0\n",
      "Game 200 score : 69.0\n",
      "Game 300 score : 109.0\n",
      "Game 400 score : 117.0\n",
      "Game 500 score : 67.0\n",
      "Game 600 score : 242.0\n",
      "Game 700 score : 230.0\n",
      "Game 800 score : 209.0\n",
      "Game 900 score : 121.0\n",
      "Game 1000 score : 244.0\n",
      "Game 1100 score : 195.0\n",
      "Game 1200 score : 137.0\n",
      "Game 1300 score : 41.0\n",
      "Game 1400 score : 89.0\n",
      "Game 1500 score : 292.0\n",
      "Game 1600 score : 218.0\n",
      "Game 1700 score : 33.0\n",
      "Game 1800 score : 386.0\n",
      "Game 1900 score : 352.0\n",
      "Game 2000 score : 230.0\n",
      "Game 2100 score : 327.0\n",
      "Game 2200 score : 313.0\n",
      "Game 2300 score : 259.0\n",
      "Game 2400 score : 342.0\n",
      "Game 2500 score : 413.0\n",
      "Game 2600 score : 386.0\n",
      "Game 2700 score : 292.0\n",
      "Game 2800 score : 312.0\n",
      "Game 2900 score : 277.0\n",
      "Game 3000 score : 500.0\n",
      "Game 3100 score : 28.0\n",
      "Game 3200 score : 31.0\n",
      "Game 3300 score : 10.0\n",
      "Game 3400 score : 8.0\n",
      "Game 3500 score : 8.0\n",
      "Game 3600 score : 10.0\n",
      "Game 3700 score : 139.0\n",
      "Game 3800 score : 94.0\n",
      "Game 3900 score : 115.0\n",
      "Game 4000 score : 263.0\n",
      "Game 4100 score : 225.0\n",
      "Game 4200 score : 255.0\n",
      "Game 4300 score : 315.0\n",
      "Game 4400 score : 500.0\n",
      "Game 4500 score : 500.0\n",
      "Game 4600 score : 500.0\n",
      "Game 4700 score : 500.0\n",
      "Game 4800 score : 371.0\n",
      "Game 4900 score : 241.0\n",
      "Game 5000 score : 292.0\n",
      "Game 5100 score : 500.0\n",
      "Game 5200 score : 255.0\n",
      "Game 5300 score : 500.0\n",
      "Game 5400 score : 500.0\n",
      "Game 5500 score : 500.0\n",
      "Game 5600 score : 333.0\n",
      "Game 5700 score : 143.0\n",
      "Game 5800 score : 233.0\n",
      "Game 5900 score : 500.0\n",
      "Game 6000 score : 408.0\n",
      "Game 6100 score : 500.0\n",
      "Game 6200 score : 500.0\n",
      "Game 6300 score : 500.0\n",
      "Game 6400 score : 184.0\n",
      "Game 6500 score : 500.0\n",
      "Game 6600 score : 10.0\n",
      "Game 6700 score : 16.0\n",
      "Game 6800 score : 72.0\n",
      "Game 6900 score : 187.0\n",
      "Game 7000 score : 150.0\n",
      "Game 7100 score : 500.0\n",
      "Game 7200 score : 194.0\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "game_memory = deque(maxlen=100000)\n",
    "\n",
    "N = 100\n",
    "last_N_scores = deque(maxlen=N)\n",
    "SCORE_REQUIRED = 490\n",
    "\n",
    "goal_steps = 500\n",
    "TRAIN_EPISODES = 100000\n",
    "\n",
    "train_size = 32\n",
    "update_every = 1\n",
    "\n",
    "epsilon = 1.0 #Defines the exploration range\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.99\n",
    "gamma = 0.99\n",
    "\n",
    "for game in range(TRAIN_EPISODES):\n",
    "    state = env.reset()\n",
    "    \n",
    "    if game % update_every == 0:\n",
    "        target_model.set_weights(model.get_weights())\n",
    "    \n",
    "    score = 0\n",
    "    for _ in range(goal_steps):\n",
    "        #env.render()\n",
    "        state = state.reshape(-1, len(state))\n",
    "        \n",
    "        action = next_action(state, output_size)\n",
    "                \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        score += reward\n",
    "        if done:\n",
    "            reward = -reward\n",
    "        else:\n",
    "            reward = reward\n",
    "        \n",
    "        game_memory.append((state, reward, done, new_state.reshape(-1, len(new_state)), action))\n",
    "        \n",
    "        state = new_state\n",
    "        if done: \n",
    "            #print(\"episode: {}/{}, score: {}, e: {:.2}\".format(game, EPISODES, _, epsilon))\n",
    "            break\n",
    "            \n",
    "    #complete training if last N episodes yield more than required score\n",
    "    last_N_scores.append(score)\n",
    "    if(sum(last_N_scores)/N >= SCORE_REQUIRED):\n",
    "        print('Score {} achieved at {} game'.format(SCORE_REQUIRED, game))\n",
    "        break\n",
    "            \n",
    "    if(len(game_memory) > train_size):\n",
    "        replay(model, game_memory, train_size)\n",
    "    \n",
    "    if(game % 100 == 0):\n",
    "        print('Game {} score : {}'.format(game, score))\n",
    "        scores.append(score)\n",
    "        \n",
    "    \n",
    "if(len(scores) > 0):\n",
    "    print('Average Score taken every 100 steps:',sum(scores)/len(scores))\n",
    "else:\n",
    "    print('Average Score:', SCORE_REQUIRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_res(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_EPISODES = 100\n",
    "goal_steps = 500\n",
    "epsilon = 0.0\n",
    "output_size = 2\n",
    "\n",
    "scores = []\n",
    "for episode in range(TEST_EPISODES):\n",
    "    state = env.reset()\n",
    "    \n",
    "    score = 0\n",
    "    for _ in range(goal_steps):\n",
    "        env.render()\n",
    "        \n",
    "        state = state.reshape(-1, len(state))\n",
    "        action = next_action(state, output_size)\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        \n",
    "        state = new_state\n",
    "        if(done):\n",
    "            break\n",
    "            \n",
    "    scores.append(score)\n",
    "        \n",
    "plot_res(scores)        \n",
    "print(\"Average score over {} episodes: {}\".format(TEST_EPISODES, sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python mlproject",
   "language": "python",
   "name": "mlproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
